
# Statistiques bayésiennes - Comprendre le modèle génératif
####################################
#
set.seed(1)
random_prob = runif(1, min = 0, max = 1)
print(random_prob)

prop_success = 0.2
print(random_prob < prop_success)

n_samples = 10
data = c()
for(sample_idx in 1:n_samples) {
  data[sample_idx] <- runif(1, min = 0, max = 1) < prop_success
}
print(data)

data = as.numeric(data)
print(data)

set.seed(1)
print(rbinom(n = n_samples, size = 1, prob = prop_success))

####################################
#Comprendre les distributions a priori
set.seed(1)
prop_successes = runif(n_samples, min = 0.0, max = 0.2)
print(prop_successes)

library(ggplot2)
# Échantillonne 1000 tirages de la priorité Beta(35,55)
prior_A = rbeta(n = 1000, shape1 = 35, shape2 = 55)
# Stocke les résultats dans un dataframe
prior_sim = data.frame(prior_A)
# Construit un graphique de densité de l'échantillon a priori
ggplot(prior_sim, aes(x = prior_A)) +
  geom_density()

# Échantillonne des tirages des a priori Beta(1,1) et Beta(100,100)
prior_B = rbeta(n = 1000, shape1 = 1, shape2 = 1)
prior_C = rbeta(n = 1000, shape1 = 100, shape2 = 100)
# Combine les résultats dans un seul dataframe
prior_all = data.frame(samples = c(prior_A, prior_B, prior_C),
                       priors = rep(c("A","B","C"), each = 1000))
# Trace les 3 a priori
ggplot(prior_all, aes(x = samples, fill = priors)) +
  geom_density(alpha = 0.5)

####################################
#Introduction à la fonction de vraisemblance
# Données observées
x = c(1, 2, 3, 4, 5)
y = c(2, 3, 5, 6, 7)
# Valeur du paramètre
b = 0.8
# Calcule les valeurs prédites
y_pred = b * x
# Calcule les résidus
residuals = y - y_pred

log_likelihood = -0.5 * length(y) * log(2 * pi) - 0.5 * sum(residuals^2)
print(log_likelihood)

install.packages("ggridges")
library(ggridges)
# Définit un vecteur de 1000 valeurs p
p_grid = seq(from = 0, to = 1, length.out = 1000)
# Simule 10 essais pour chaque p dans p_grid, chaque essai a 1000 échantillons
sim_result = rbinom(n = 1000, size = 10, prob = p_grid)
# Collecte les résultats dans un dataframe
likelihood_sim = data.frame(p_grid, sim_result)
# Graphiques de densité de p_grid groupés par sim_result
ggplot(likelihood_sim, aes(x = p_grid, y = sim_result, group = sim_result)) +
  geom_density_ridges()

####################################
####################################
#Introduction au modèle a posteriori
install.packages("JAGS")
install.packages("rjags")
library(rjags)
# Définir le modèle
bayes_model = "model{
    # Modèle de vraisemblance pour X
    X ~ dbin(p, n)
    # Modèle a priori pour p
    p ~ dbeta(a, b)
}"
# Compiler le modèle
bayes_jags = jags.model(textConnection(bayes_model),
                        data = list(a = 1, b = 1, X = 3, n = 10))

# Simuler l'a posteriori
bayes_sim = coda.samples(model = bayes_jags, variable.names = c("p"), n.iter = 10000)

# Tracer l'a posteriori
plot(bayes_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,3))

####################################
#Introduction au modèle normal-normal
library(coda)
set.seed(1)
mu_true = 2
sd_true = 1
n = 100
data = rnorm(n, mean = mu_true, sd = sd_true)

model_string = "model {
    for (i in 1:n) {
        y[i] ~ dnorm(mu, prec)
    }
    mu ~ dnorm(0, 0.1)
    sigma ~ dunif(0, 10)
    prec <- pow(sigma, -2)
}"

data_jags = list(y = data, n = n)
model = jags.model(textConnection(model_string), data = data_jags)
update(model, 1000)  # période de chauffe

params = c("mu", "sigma")
samples = coda.samples(model, params, n.iter = 10000)
# Imprimer les statistiques sommaires pour les échantillons a posteriori
print(summary(samples))

print(plot(samples))

####################################
#Introduction à MCMC (Monte Carlo Markov Chains)
# Stocker les chaînes dans un dataframe
mcmc_chains <- data.frame(samples[[1]], iter = 1:10000)
# Examiner les premières lignes
head(mcmc_chains)

# Utiliser plot() pour construire des graphiques de trace
plot(samples, density = FALSE)

# Graphique de trace des 100 premières itérations de la chaîne pour mu
ggplot(mcmc_chains[1:100, ], aes(x = iter, y = mu)) +
  geom_line() +
  theme(axis.title.x = element_text(size = 20),  # Augmenter la taille des étiquettes de l'axe x
        axis.title.y = element_text(size = 20))  # Augmenter la taille des étiquettes de l'axe y

# Utiliser plot() pour construire des graphiques de densité
plot(samples, trace = FALSE)

model2 = jags.model(textConnection(model_string), data = data_jags, n.chains = 4)
# simuler l'a posteriori
samples2 <- coda.samples(model = model2, variable.names = params, n.iter = 1000)

# Construire des graphiques de trace
plot(samples2, density = FALSE)

summary(samples2)

####################################
#La procédure complète d'inférence bayésienne
# charger les bibliothèques nécessaires
library(rjags)
library(coda)
# définir le modèle
model = "model{
    # Définir le modèle pour les données Y[i]
    for(i in 1:length(Y)) {
      Y[i] ~ dnorm(m[i], s^(-2))
      m[i] <- a + b * X[i]
    }
    # Définir les priors pour a, b, s
    a ~ dnorm(0, 0.5^(-2))
    b ~ dnorm(1, 0.5^(-2))
    s ~ dunif(0, 20)
}"

# compiler le modèle
model = jags.model(textConnection(model),
                   data = list(Y = mtcars$wt, X = mtcars$hp),
                   n.chains = 3,
                   inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# période de chauffe
update(model, 1000)

# générer des échantillons MCMC
samples = coda.samples(model, variable.names = c("a", "b", "s"), n.iter = 5000)

# vérifier la convergence en utilisant le graphique de trace
plot(samples)

# Obtenir les estimations a posteriori
posterior_estimates = summary(samples)
# Calculer la moyenne pour chaque paramètre
a_mean = posterior_estimates$statistics["a", "Mean"]
b_mean = posterior_estimates$statistics["b", "Mean"]
# Tracer la ligne de prédiction
ggplot(mtcars, aes(x = hp, y = wt)) +
  geom_point() +
  geom_abline(intercept = a_mean, slope = b_mean) +
  labs(title = "Régression Linéaire Bayésienne",
       x = "Puissance (hp)",
       y = "Poids (wt)") +
  theme(plot.title = element_text(hjust = 0.5))

# Extraire des échantillons
a_samples = as.matrix(samples[, "a"])
b_samples = as.matrix(samples[, "b"])
# Calculer les intervalles crédibles
a_hpd = coda::HPDinterval(coda::as.mcmc(a_samples))
b_hpd = coda::HPDinterval(coda::as.mcmc(b_samples))
# Tracer des histogrammes et des intervalles crédibles
par(mfrow=c(2,1))  # Créer 2 sous-graphiques
# Paramètre a
hist(a_samples, freq=FALSE, xlab="a", main="Distribution a posteriori de a", col="lightgray")
abline(v=a_hpd[1,1], col="red", lwd=2)  # Limite inférieure de l'intervalle crédible
abline(v=a_hpd[1,2], col="red", lwd=2)  # Limite supérieure de l'intervalle crédible
# Paramètre b
hist(b_samples, freq=FALSE, xlab="b", main="Distribution a posteriori de b", col="lightgray")
abline(v=b_hpd[1,1], col="red", lwd=2)  # Limite inférieure de l'intervalle crédible
abline(v=b_hpd[1,2], col="red", lwd=2)  # Limite supérieure de l'intervalle crédible
